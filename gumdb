#!/usr/bin/env python3
#
# Grand Unified Mail Database
# Copyright (C) 2018 Richard Brooksby
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Grand Unified Mail Database"""

import argparse
import email
import email.parser
import email.policy
import email.utils
import logging
import mailbox
import os.path
import pathlib
import pyzor.digest
import re
import sqlite3
import sys
import time
import uuid

logger = logging.getLogger("gumdb")
logger.setLevel(logging.WARNING)
logger.addHandler(logging.StreamHandler())


# We'd like to use the "default" policy (which isn't the default)
# and take advantage of its structured header classes, but
# <https://bugs.python.org/issue35342> makes it unusable.
email_parser = email.parser.BytesParser(policy = email.policy.compat32)


def schema(conn):
    
    # Core schema
    
    # WITHOUT ROWID is possible for the messages table, but not
    # recommended because it has large rows.  See
    # <https://sqlite.org/withoutrowid.html>.
    conn.execute('''
        CREATE TABLE IF NOT EXISTS messages (
            id TEXT PRIMARY KEY NOT NULL,
            message BLOB NOT NULL
        )
    ''')

    conn.execute('''
        CREATE TABLE IF NOT EXISTS sources (
            id TEXT
                PRIMARY KEY
                NOT NULL
                REFERENCES messages(id)
                ON DELETE CASCADE,
            type TEXT NOT NULL,
            file TEXT,
            key TEXT,
            mtime FLOAT,
            UNIQUE (type, file, key, mtime)
        )
    ''')
    # The sources_tkf index is important for checking whether a
    # message has already been added, or has been added in an earlier
    # version of a mailbox.
    conn.execute('''
        CREATE INDEX IF NOT EXISTS sources_tfk
        ON sources (type, file, key)
    ''')

    # headers.id can't be a primary key because messages have more
    # than one header.  (headers.id, headers.name) can't be a primary
    # key because messages can have more than one header with the same
    # name (and possibly even the same value).  So the headers table
    # is a real bag, not a set.
    conn.execute('''
        CREATE TABLE IF NOT EXISTS headers (
            id TEXT
              NOT NULL
              REFERENCES messages(id)
              ON DELETE CASCADE,
            name TEXT NOT NULL,
            value TEXT NOT NULL
        )
    ''')
    conn.execute('CREATE INDEX IF NOT EXISTS headers_id ON headers (id)')

    # (addresses.id, addresses.header) can't be a primary key because
    # messages may have more than one address header with the same
    # name (and value), e.g. Two "To" headers.
    conn.execute('''
        CREATE TABLE IF NOT EXISTS addresses (
            id TEXT
              NOT NULL
              REFERENCES messages(id)
              ON DELETE CASCADE,
            header TEXT NOT NULL,
            name TEXT,
            address TEXT NOT NULL
        )
    ''')
    conn.execute('CREATE INDEX IF NOT EXISTS addresses_id on addresses (id)')

    conn.execute('''
        CREATE TABLE IF NOT EXISTS dates (
            id TEXT
              NOT NULL
              REFERENCES messages(id)
              ON DELETE CASCADE,
            header TEXT NOT NULL,
            date TEXT NOT NULL
        )
    ''')
    conn.execute('CREATE INDEX IF NOT EXISTS dates_id on dates (id)')

    conn.execute('''
        CREATE TABLE IF NOT EXISTS envelopes (
            id TEXT
              NOT NULL
              PRIMARY KEY
              REFERENCES messages(id)
              ON DELETE CASCADE,
            unixfrom TEXT NOT NULL
        ) WITHOUT ROWID
    ''')

    # TODO: Scans should probably have a type, just to catch errors
    # where a file is scanned as one type, then another.
    conn.execute('''
        CREATE TABLE IF NOT EXISTS scans (
            file TEXT NOT NULL,
            mtime FLOAT NOT NULL
        )
    ''')

    # Digestify schema

    conn.execute('''
        CREATE TABLE IF NOT EXISTS digests (
            id TEXT
                NOT NULL
                PRIMARY KEY
                REFERENCES messages (id)
                ON DELETE CASCADE,
            digest TEXT NOT NULL
        )
    ''')

    conn.execute('''
        CREATE VIEW IF NOT EXISTS duplicates AS
            SELECT id, digest
            FROM (SELECT digest
                  FROM digests
                  GROUP BY digest
                  HAVING count(*) > 1)
            LEFT JOIN digests USING (digest)
    ''')
    conn.execute('''
        CREATE INDEX IF NOT EXISTS digest_digests ON digests (digest)
    ''')
    conn.execute('''
        CREATE INDEX IF NOT EXISTS headers_message_ids ON headers (value) WHERE name = 'message-id'
    ''')


def connect(path):
    """Connect to sqlite database with required settings and ensure schema is present."""
    # TODO: Make sure it's not autocommit
    conn = sqlite3.connect(path, timeout=60)
    # See <https://sqlite.org/foreignkeys.html>.
    if conn.execute('PRAGMA foreign_keys'):
        conn.execute('PRAGMA foreign_keys = ON')
    else:
        logger.warn('Foreign keys not supported in sqlite3. Database may accumulate cruft.')
    schema(conn)
    return conn


def fetch(cursor):
    """Convert a cursor into an iterator."""
    while True:
        rows = cursor.fetchmany()
        if not rows: return
        yield from rows


def insert_message(conn, message_bytes, source = None):
    """Insert a single message into the database."""
    
    message = email_parser.parsebytes(message_bytes)

    logger.info('Adding message from %s date %s subject %s',
                repr(str(message['from'])),
                repr(str(message['date'])),
                repr(str(message['subject'])))

    id = uuid.uuid4().hex
    conn.execute('INSERT INTO messages (id, message) VALUES (?, ?)', (id, message_bytes))

    if source:
        source['id'] = id
        fields = ['id', 'type', 'file', 'key', 'mtime']
        conn.execute(
            f'''
               INSERT INTO sources ({','.join(fields)})
               VALUES ({','.join(':'+f for f in fields)})
            ''',
            {f:source.get(f) for f in fields}
        )

    conn.executemany('INSERT INTO headers (id, name, value) VALUES (?, ?, ?)',
                     [(id, name.lower(), str(value)) for name, value in message.items()])

    # See
    # <https://docs.python.org/3.7/library/email.headerregistry.html#email.headerregistry.HeaderRegistry>
    # for the origin of this list of headers.
    values = []
    for header in ['to', 'from', 'cc', 'bcc', 'resent-to', 'resent-from',
                   'sender', 'resent-sender', 'reply-to']:
        addresses = email.utils.getaddresses([str(h) for h in (message.get_all(header) or ())])
        for name, address in addresses:
            if address != '':
                values.append((id, header, name, address))
    conn.executemany('INSERT INTO addresses (id, header, name, address) VALUES (?, ?, ?, ?)', values)

    # See
    # <https://docs.python.org/3.7/library/email.headerregistry.html#email.headerregistry.HeaderRegistry>
    # for the origin of this list of headers.
    values = []
    for header in ['date', 'resent-date', 'orig-date']:
        for value in (message.get_all(header) or ()):
            try:
                # TODO: Docs don't say what date.datetime is if the
                # header isn't parseable.  It seems to just break in
                # undocumented ways.  See
                # <https://bugs.python.org/issue30681>.
                values.append((id, header, email.utils.parsedate_to_datetime(value)))
            except:
                pass
    conn.executemany('INSERT INTO dates (id, header, date) VALUES (?, ?, ?)', values)

    unixfrom = message.get_unixfrom()
    if unixfrom:
        conn.execute('INSERT INTO envelopes (id, unixfrom) VALUES (?, ?)', (id, unixfrom))

    try:
        hex = digest_message(message)
    except:
        logger.error('Failed to digest message %s', id)
        # TODO: Traceback
    else:
        logger.debug('%s digest: %s', id, hex)
        conn.execute('INSERT INTO digests (id, digest) VALUES (?, ?)', (id, hex))

    conn.commit()


def maybe_insert_message(conn, message_bytes, source = None):
    """Insert a message into the database if we don't already have it from the same source."""
    if source and 'file' in source and 'mtime' in source:
        type = source['type']
        path = source['file']
        key = source.get('key')
        mtime = source['mtime']
        # TODO: Consider loosening criteria to just type and path,
        # then "stealing" the existing message from the other version
        # of the file, reducing database and id churn.
        cur = conn.execute("""
            SELECT id, digest
            FROM sources LEFT NATURAL JOIN digests
            WHERE type = ? AND file = ? AND key = ? AND mtime = ?
        """, (type, path, key, mtime))
        digest = None
        # There should only be zero or one pf these, but just in case...
        for existing_id, existing_digest in fetch(cur):
            logger.debug('Found existing source duplicate message %s from %s[%s] at %s', existing_id, path, key, mtime)
            if not digest:
                digest = digest_message(email_parser.parsebytes(message_bytes))
            if not existing_digest:
                existing_bytes = conn.execute("SELECT message FROM messages WHERE id = ?", (existing_id,)).fetchone()[0]
                existing_message = email_parser.parsebytes(existing_bytes)
                existing_digest = digest_message(existing_message)
                logger.debug('Adding digest for existing message %s: %s', existing_id, existing_digest)
                conn.execute("INSERT INTO digests (id, digest) VALUES (?, ?)", (existing_id, existing_digest))
                conn.commit()
            if digest != existing_digest:
                logger.error('Existing source duplicate %s digest %s differs from new version %s.', existing_id, existing_digest, digest)
            else:
                return False
    insert_message(conn, message_bytes, source = source)
    return True
    

def insert(args, prefix = 'gumdb'):
    """Insert messages into the database from the command line."""
    parser = argparse.ArgumentParser(prog = prefix + ' insert',
                                     description = 'Insert email messages into the database.')
    parser.add_argument('--mbox',
                        action = 'store_true',
                        help = 'accept a mailbox of messages in mbox format')
    parser.add_argument('--dots',
                        action = 'store_true',
                        help = 'print one dot per message to standard output to show progress')
    parser.add_argument('path',
                        metavar = '<path>',
                        nargs = '+',
                        help = 'one or more files from which to read messages')
    insert_args = parser.parse_args(args.arguments)
    conn = connect(str(args.database[0]))
    for path in insert_args.path:
        if path == '-':
            mtime = time.time()
        else:
            mtime = os.path.getmtime(path)
            if conn.execute("SELECT 1 FROM scans WHERE file = ? AND mtime = ?", (path, mtime)).fetchall():
                logger.info('File %s already scanned.', path)
                continue

        box = None
        if insert_args.mbox:
            type = 'mbox'
            box = mailbox.mbox(path, create = False)
        # TODO: Insert other kinds of mailbox here

        if box:
            # TODO: locking?
            for key in box.iterkeys():
                message_bytes = box.get_bytes(key, from_ = True)
                if maybe_insert_message(conn,
                                        message_bytes,
                                        source = dict(type = type,
                                                      file = path,
                                                      key = key,
                                                      mtime = mtime)):
                    dot = '.'
                else:
                    dot = 'd'
                if insert_args.dots: print(dot, end = '', flush = True)
            if insert_args.dots: print(flush = True)
        else:
            if path == '-':
                insert_message(conn,
                               sys.stdin.buffer.read(),
                               source = dict(type = 'file',
                                             mtime = mtime))
            else:
                maybe_insert_message(conn,
                                     open(path, 'rb').read(),
                                     source = dict(type = 'file',
                                                   file = path,
                                                   mtime = mtime))

        conn.execute("INSERT INTO scans (file, mtime) VALUES (?, ?)", (path, mtime))
        conn.commit()

        # Delete messages that were in an earlier version of
        # the mailbox that are in the new version.  Keep
        # messages that aren't in the new version.  TODO:
        # Explain why we keep them.
        count = conn.execute("""
            DELETE FROM messages
            WHERE id IN (SELECT older_sources.id
                         FROM sources
                         JOIN digests USING (id)
                         LEFT JOIN digests AS other_digests USING (digest)
                         JOIN sources AS older_sources
                             ON older_sources.id = other_digests.id AND
                                older_sources.file = sources.file
                         WHERE sources.file = ? AND sources.mtime = ? AND
                               (older_sources.mtime IS NULL OR older_sources.mtime < sources.mtime))
        """, (path, mtime)).rowcount
        logger.info('Deleted %d messages from earlier versions of %s', count, path)
        conn.commit()


def digest_message(message):
    """Digest a message into a hex string that compares equal only for duplicate messages."""
    digest = pyzor.digest.DataDigester(message).digest
    # Pyzor's digest doesn't include Message-Ids.  We want to
    # distinguish identical messages with different Message-Ids,
    # so add them to the digest.
    for message_id in message.get_all('message-id') or ():
        digest.update(str(message_id).encode('utf-8'))
    return digest.hexdigest()

    
def digestify(args, prefix = 'gumdb'):
    """Add digests to messages."""
    parser = argparse.ArgumentParser(prog = prefix + ' digestify',
                                     description = 'Add digests to messages that might be duplicates.')
    parser.add_argument('id',
                        nargs = '+',
                        help = 'GUMDB id of messages to digest')
    digestify_args = parser.parse_args(args.arguments)
    conn = connect(str(args.database[0]))

    for id in digestify_args.id:
        row = conn.execute("SELECT message, digest FROM messages NATURAL LEFT JOIN digests WHERE id = ?", (id,)).fetchone()
        if not row:
            logger.error('No message with id %s', id)
            continue
        message_bytes, digest = row
        message = email_parser.parsebytes(message_bytes)
        try:
            hex = digest_message(message)
        except:
            logger.error('Failed to digest message %s', id)
        else:
            if digest:
                if hex != digest:
                    logger.error('Replacing digest %s with %s for message %s', digest, hex, id)
            else:
                logger.info('Adding digest for %s: %s', id, hex)
                conn.execute('INSERT OR REPLACE INTO digests (id, digest) VALUES (?, ?)', (id, hex))
                conn.commit()


def main():
    commands = {
        'help': lambda args, prefix = 'help': parser.print_help(),
        'insert': insert,
        'digestify': digestify,
    }
    parser = argparse.ArgumentParser(description = 'Grand Unified Mail Database')
    default_database = pathlib.Path.home() / 'var/gumdb.sqlite3'
    parser.add_argument('-f', '--database',
                        nargs = 1,
                        default = [default_database],
                        help = 'path to database file (default: %s)' % default_database)
    parser.add_argument('-d', '--debug',
                        action = 'store_true',
                        help = 'enable debugging output (logging level DEBUG)')
    parser.add_argument('-v', '--verbose',
                        action = 'store_true',
                        help = 'enable verbose output (logging level INFO)')
    parser.add_argument('command',
                        metavar = '<command>',
                        help = ', '.join(commands.keys()))
    parser.add_argument('arguments',
                        metavar = '...',
                        nargs = argparse.REMAINDER,
                        help = 'Pass --help to each command for detailed help.')
    args = parser.parse_args()
    if args.verbose:
        logger.setLevel(logging.INFO)
    if args.debug:
        logger.setLevel(logging.DEBUG)
    logger.debug('Debug logging enabled')
    prefix = re.sub(r'(?ims)\Ausage: (.*?) <command>.*\Z', r'\1', parser.format_usage())
    try:
        commands[args.command](args, prefix = prefix)
    except KeyError:
        logger.error('Unknown command %s' % repr(args.command))
        sys.exit(1)

if __name__ == '__main__':
    main()
